# ğŸ‘‹ Hi, I'm Jiahao Zhang (å¼ ç”²è±ª)

ğŸ“ Research Assistant @ HKUST (Guangzhou)  
ğŸ”¬ Focus: Multimodal Large Models, Cross-modal Retrieval, Video-Audio Understanding

---

## ğŸ§  Current Research

I'm currently working at HKUST (GZ) under the supervision of Prof. Xuming Hu, where I lead and contribute to research on large-scale multimodal understanding and retrieval. My recent work focuses on:

- ğŸ” **Cross-modal retrieval** using Cauchy-Schwarz divergence (CS/GCS)
- ğŸ¥ **Video-audio-text understanding** with spatiotemporal graphs
- ğŸ” **Fine-grained multimodal alignment** and contrastive learning
- ğŸ§© **Causal reasoning and storyline modeling** in long videos
- ğŸ§  Integration of large vision-language models like **Qwen-VL 32B**

I aim to build more interpretable, generalizable, and scalable solutions for multimodal representation learning.

---

## ğŸ› ï¸ Tech Stack

**Languages**: Python, MATLAB  
**Frameworks**: PyTorch, Hugging Face Transformers  
**Toolkits**: Git, LaTeX, t-SNE, FFmpeg  
**Domains**: Multimodal Retrieval, Video Understanding, Audio Representation, Generative Alignment  
**Concepts**: CS Divergence, Temporal Graphs, Prompt Tuning, RAG (Retrieval-Augmented Generation)

---

## ğŸ“ˆ Highlights

- ğŸ“ **ACM Multimedia 2025 submission**: Proposed GCS-based tri-modal retrieval surpassing state-of-the-art
- ğŸ§® Built **VST framework** for spatiotemporal entity-relation modeling in long videos
- ğŸ§ Designed multi-scale audio transformers for event and speech disentanglement
- ğŸ’¬ Developed bidirectional prompt fusion for cross-modal tuning and causal inference

---

## ğŸ“¬ Contact

- ğŸ“§ Email: 6819391@qq.com  
- ğŸŒ [My LinkedIn](https://www.linkedin.com/in/your-profile) *(replace with real link)*  
- ğŸ§  [My Google Scholar](https://scholar.google.com/citations?user=XXXX) *(optional)*  

---

## ğŸ“Š GitHub Stats

![Zhang Jiahao's GitHub stats](https://github-readme-stats.vercel.app/api?username=your-github-username&show_icons=true&theme=default)
