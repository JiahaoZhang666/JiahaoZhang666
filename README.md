# 👋 Hi, I'm Jiahao Zhang (张甲豪)

🎓 Research Assistant @ HKUST (Guangzhou)  
🔬 Focus: Multimodal Large Models, Cross-modal Retrieval, Video-Audio Understanding

---

## 🧠 Current Research

I'm currently working at HKUST (GZ) under the supervision of Prof. Xuming Hu, where I lead and contribute to research on large-scale multimodal understanding and retrieval. My recent work focuses on:

- 🔁 **Cross-modal retrieval** using Cauchy-Schwarz divergence (CS/GCS)
- 🎥 **Video-audio-text understanding** with spatiotemporal graphs
- 🔍 **Fine-grained multimodal alignment** and contrastive learning
- 🧩 **Causal reasoning and storyline modeling** in long videos
- 🧠 Integration of large vision-language models like **Qwen-VL 32B**

I aim to build more interpretable, generalizable, and scalable solutions for multimodal representation learning.

---

## 🛠️ Tech Stack

**Languages**: Python, MATLAB  
**Frameworks**: PyTorch, Hugging Face Transformers  
**Toolkits**: Git, LaTeX, t-SNE, FFmpeg  
**Domains**: Multimodal Retrieval, Video Understanding, Audio Representation, Generative Alignment  
**Concepts**: CS Divergence, Temporal Graphs, Prompt Tuning, RAG (Retrieval-Augmented Generation)

---

## 📈 Highlights

- 📝 **ACM Multimedia 2025 submission**: Proposed GCS-based tri-modal retrieval surpassing state-of-the-art
- 🧮 Built **VST framework** for spatiotemporal entity-relation modeling in long videos
- 🎧 Designed multi-scale audio transformers for event and speech disentanglement
- 💬 Developed bidirectional prompt fusion for cross-modal tuning and causal inference

---

## 📬 Contact

- 📧 Email: 6819391@qq.com  
- 🌐 [My LinkedIn](https://www.linkedin.com/in/your-profile) *(replace with real link)*  
- 🧠 [My Google Scholar](https://scholar.google.com/citations?user=XXXX) *(optional)*  

---

## 📊 GitHub Stats

![Zhang Jiahao's GitHub stats](https://github-readme-stats.vercel.app/api?username=your-github-username&show_icons=true&theme=default)
